1. login: 

   ssh username@orcd-login001.mit.edu 

2. code
   git clone git@github.com:PrincetonUniversity/M3DC1.git 
   git clone -b [branch] git@github.com:PrincetonUniversity/M3DC1.git 
   git commit -m "mit code porting"
   git push origin master

3. load modules and set environment

   for development modules:
       export M3DC1_CODE_DIR=$HOME/src/M3DC1  [for example]
       module use $M3DC1_CODE_DIR/unstructured/modules/mit
       module load m3dc1/devel-gcc

   or

   module load openmpi/5.0.8

4. compile code

   make ARCH=mit_gcc all
    (make OPT=1 RL=1 MAX_PTS=25 ARCH=mit_gcc
     make OPT=1 COM=1 MAX_PTS=25 ARCH=mit_gcc
     make 3D=1 OPT=1 MAX_PTS=60 ARCH=mit_gcc
     make 3D=1 OPT=1 MAX_PTS=60 ARCH=mit_gcc
    )

5. mesh utility

5.1 mesh generation
  /orcd/nese/psfc/001/scorec/openmpi5.0.8-gcc12.2.0/2025.0-250318dev/bin/m3dc1_meshgen

  e.g. salloc -p sched_mit_psfc_r8 --nodes 1 --ntasks-per-node=1 -t 00:10:00
       srun -n 1 ./m3dc1_meshgen M3DC1/unstructured/tutorials/Meshgen/input7
  
  The input file must use the multi-region model format, which begins with "numBdry".

  Model files (.txt) generated using the old (deprecated) format are not compatible with M3DC1 and will fail to run.
  To convert an existing old-format model file to the new format, append the following lines to the end of the file:

  [FIXED WALL MODELS] 
  1
  1 1 1


  [RESISTIVE WALL MODELS] 
  3
  1 1 1
  2 2 1 2
  3 2 2 3

  For more details, see readme and M3DC1 User's Guide (M3DC1/doc/M3DC1.tex)

5.2 mesh partitioning and merging
  see $SCOREC_UTIL_DIR in mit_gcc.mk

6. options_bjacobi for 3D

        -pc_type bjacobi
        -pc_bjacobi_blocks 4
        -sub_pc_type lu
        -sub_pc_factor_mat_solver_type mumps
        -sub_mat_mumps_icntl_14 100
        -sub_ksp_type preonly
        -ksp_type fgmres
        -ksp_gmres_restart 220
        -ksp_rtol 1.e-9
        -ksp_atol 1.e-20
        -ksp_converged_reason

        -hard_pc_type bjacobi
        -hard_pc_bjacobi_blocks 4
        -hard_sub_pc_type lu
        -hard_sub_pc_factor_mat_solver_type mumps
        -hard_sub_mat_mumps_icntl_14 100
        -hard_sub_ksp_type preonly
        -hard_ksp_type fgmres
        -hard_ksp_gmres_restart 220
        -hard_ksp_rtol 1.e-9
        -hard_ksp_atol 1.e-20
        -hard_ksp_converged_reason

        -on_error_abort

   Please change the number of blocks (here 4) to match the number of plane in your 'C1input' file.

   We have another set of options with 'hard_" as the prefix for PETSC VERSION 3.8 or newer
   to isolate the hard solves (#5, #17) from easier ones for optimization purpose.

7. run job

7.1 interactive mode
  salloc -n 48 -t 00:30:00 -p sched_mit_psfc_r8
  export SLURM_CPU_BIND="cores"
  srun ... (see 7.2)

7.2 batch mode
   sample slurm script "batchjob.mit_gcc"

   #!/bin/bash
   #SBATCH -p sched_mit_psfc_r8
   #SBATCH -n 48
   #SBATCH -J m3dc1_regtest_adapt
   #SBATCH --time 0:30:00
   #SBATCH -o C1stdout

   export SLURM_CPU_BIND="cores"

   srun -n 32 -c 8 m3dc1_2d_complex -pc_factor_mat_solver_type mumps
   srun -n 32 -c 8 m3dc1_2d -pc_factor_mat_solver_type mumps
   srun -n 32 -c 8 m3dc1_3d -options_file options_bjacobi.type_mumps

8. regression tests
   cd unstructured
   export M3DC1_MPIRUN=srun M3DC1_VERSION=local M3DC1_ARCH=mit_gcc
   make bin ARCH=$M3DC1_ARCH
   cd _mit_gcc/bin/; export PATH=`pwd`:$PATH
   cd ../../regtest/
   ./run $M3DC1_ARCH
   ./check $M3DC1_ARCH


